{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "### Name: Anuguru Parthiv Reddy\n",
    "### Roll Number: 21CS10006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary libraries here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../dataset/Pumpkin_Seeds_Dataset.csv')\n",
    "data.isnull().sum()             #to check for null values\n",
    "data['Class'].unique()          #removes unique values to see number of classes\n",
    "data['Class'].value_counts()    #to see count of each class \n",
    "le = LabelEncoder()\n",
    "data[\"Class\"] = le.fit_transform(data[\"Class\"]) #used to assign numerical values(0-bad,1-good) to the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into features (x_data) and target labels (y_data)\n",
    "x_data = data.iloc[:, :-1]\n",
    "y_data = data.iloc[:, -1]\n",
    "\n",
    "# Standardizing the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_x_data = scaler.fit_transform(x_data)\n",
    "scaled_x_data = pd.DataFrame(scaled_x_data)\n",
    "scaled_x_data.columns = x_data.columns\n",
    "\n",
    "# Splitting the data into train, validation, and test sets\n",
    "x_train, x_rem, y_train, y_rem = train_test_split(scaled_x_data, y_data, train_size=0.5)  # 50% for training\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_rem, y_rem, train_size=0.6)           # 60% of remaining data for validation\n",
    "\n",
    "# Resetting and dropping the index column for consistency in data handling\n",
    "x_train = x_train.reset_index()\n",
    "y_train = y_train.reset_index()\n",
    "x_val = x_val.reset_index()\n",
    "y_val = y_val.reset_index()\n",
    "x_train = x_train.drop(['index'], axis=1)\n",
    "y_train = y_train.drop(['index'], axis=1)\n",
    "x_val = x_val.drop(['index'], axis=1)\n",
    "y_val = y_val.drop(['index'], axis=1)\n",
    "y_val = y_val.iloc[:, 0]  # Extracting the Series from DataFrame\n",
    "\n",
    "# Definition of the sigmoid function for logistic regression\n",
    "def sigmoid(w, x, b):\n",
    "    ans = 0\n",
    "    for i in range(len(x)):\n",
    "        ans = ans + (w[i] * x[i])\n",
    "    return 1 / (1 + np.exp(-ans))\n",
    "\n",
    "# Definition of the loss function (binary cross-entropy) for logistic regression\n",
    "def loss(x, y, w, b):\n",
    "    f = sigmoid(w, x, b)\n",
    "    ans1 = y * (np.log(f))\n",
    "    ans2 = (1 - y) * (np.log(1 - f))\n",
    "    return -(ans1 + ans2)\n",
    "\n",
    "# Gradient calculation for weights (w) with respect to a specific feature (j)\n",
    "def grad_w(x_data, y_data, j, w, b):\n",
    "    ans = 0\n",
    "    for i in range(len(x_data)):\n",
    "        arr = np.array(x_data.loc[i])\n",
    "        ans = ans + (sigmoid(w, arr, b) - y_data.loc[i]) * arr[j]\n",
    "    return ans / len(x_data)\n",
    "\n",
    "# Gradient calculation for bias (b)\n",
    "def grad_b(x_data, y_data, w, b):\n",
    "    ans = 0\n",
    "    for i in range(len(x_data)):\n",
    "        arr = np.array(x_data.loc[i])\n",
    "        ans = ans + (sigmoid(w, arr, b) - y_data.loc[i])\n",
    "    return ans / len(x_data)\n",
    "\n",
    "# Gradient Descent optimization to update weights (w) and bias (b)\n",
    "def grad_descent(x_data, y_data, w, b, eps, alp):\n",
    "    k = 100\n",
    "    while k:\n",
    "        for j in range(len(x_data.loc[0])):\n",
    "            w[j] = w[j] - alp * grad_w(x_data, y_data, j, w, b)\n",
    "        b = b - alp * grad_b(x_data, y_data, w, b)\n",
    "        k = k - 1\n",
    "    return b\n",
    "\n",
    "# Initializing weights (w), bias (b), learning rate (alp), and convergence threshold (eps)\n",
    "w = np.zeros(12)\n",
    "b = 0\n",
    "alp = 1\n",
    "eps = 0.5\n",
    "\n",
    "# Performing gradient descent to update bias (b)\n",
    "b = grad_descent(x_train, y_train, w, b, eps, alp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data\n",
      "class 1 ==> accuracy: 0.84375, precision: 0.9264705882352942, recall: 0.84375\n",
      "class 0 ==> accuracy: 0.9456521739130435 precision: 0.9456521739130435 recall: 0.8817567567567568\n"
     ]
    }
   ],
   "source": [
    "#accuracy precision and recall for Test set\n",
    "tp=0\n",
    "tn=0\n",
    "fp=0\n",
    "fn=0\n",
    "for i in range(len(x_test)):\n",
    "    arr=np.array(x_test.iloc[i])\n",
    "    ans=sigmoid(w,arr,b)\n",
    "    if(ans>=0.5):\n",
    "        if(y_test.iloc[i]==1):\n",
    "            tp=tp+1\n",
    "        else:\n",
    "            fp=fp+1\n",
    "    else:\n",
    "        if(y_test.iloc[i]==1):\n",
    "            fn=fn+1\n",
    "        else:\n",
    "            tn=tn+1\n",
    "            \n",
    "print(\"Test data\")\n",
    "#for class 1\n",
    "accuracy=tp/(tp+fn)\n",
    "precision=tp/(tp+fp)\n",
    "recall=tp/(tp+fn)\n",
    "print(f\"class 1 ==> accuracy: {accuracy}, precision: {precision}, recall: {recall}\")\n",
    "\n",
    "#for class 0\n",
    "accuracy=tn/(tn+fp)\n",
    "precision=tn/(tn+fp)\n",
    "recall=tn/(tn+fn)\n",
    "print(f\"class 0 ==> accuracy: {accuracy} precision: {precision} recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data\n",
      "class 1 ==> accuracy: 0.8651685393258427, precision: 0.8627450980392157, recall: 0.8651685393258427\n",
      "class 0 ==> accuracy: 0.8756345177664975 precision: 0.8756345177664975 recall: 0.8778625954198473\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation data\")\n",
    "tp=0\n",
    "tn=0\n",
    "fp=0\n",
    "fn=0\n",
    "for i in range(len(x_val)):\n",
    "    arr=np.array(x_val.iloc[i])\n",
    "    ans=sigmoid(w,arr,b)\n",
    "    if(ans>=0.5):\n",
    "        if(y_val.iloc[i]==1):\n",
    "            tp=tp+1\n",
    "        else:\n",
    "            fp=fp+1\n",
    "    else:\n",
    "        if(y_val.iloc[i]==1):\n",
    "            fn=fn+1\n",
    "        else:\n",
    "            tn=tn+1\n",
    "\n",
    "#for class 1\n",
    "accuracy=tp/(tp+fn)\n",
    "precision=tp/(tp+fp)\n",
    "recall=tp/(tp+fn)\n",
    "print(f\"class 1 ==> accuracy: {accuracy}, precision: {precision}, recall: {recall}\")\n",
    "\n",
    "#for class 0\n",
    "accuracy=tn/(tn+fp)\n",
    "precision=tn/(tn+fp)\n",
    "recall=tn/(tn+fn)\n",
    "print(f\"class 0 ==> accuracy: {accuracy} precision: {precision} recall: {recall}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
